#Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
#SPDX-License-Identifier: Apache-2.0
# Target keyspace and table in Amazon Keyspaces
TARGET_KEYSPACE=ks_test_cql_replicator
TARGET_TABLE=test_cql_replicator
# Source keyspace and table in Cassandra
SOURCE_KEYSPACE=ks_test_cql_replicator
SOURCE_TABLE=test_cql_replicator
# Cassandra <= 2.1 set it to false
CASSANDRA_JSON_SERIALIZER=true
# Specify columns that you want to replicate to the target
REPLICATED_COLUMNS=key,col0,col1,col2,col3,col4,col5,col6,col7,col8
# Support column(s) that update(s) regularly, if you set it NONE, CQLReplicator replicates only inserts.
WRITETIME_COLUMNS=col1,col2
# Pooling period from the Cassandra in seconds
POOLING_PERIOD=15
REPLICATE_WITH_TIMESTAMP=false
REPLICATE_RETRY_MAXATTEMPTS=2048
# Scale up CQLReplicator instances
REPLICATE_WITH_CORE_POOL_SIZE=4
REPLICATE_WITH_MAX_CORE_POOL_SIZE=8
# Core pool timeout in seconds
REPLICATE_WITH_CORE_POOL_TIMEOUT=360
# Use PartiQL statements to transform JSON Cassandra rows into a new Keyspaces' schema if need it, https://partiql.org/tutorial.html
# inputDocument represents the source cassandra row in the PartiQL statement
# For example, transform JSON key,col0,col1,col2,col3,col4,col5,col6,col7,col8 to JSON id, col0
TRANSFORM_INBOUND_REQUEST=false
TRANSFORM_SQL=SELECT \"key\" as \"id\", \"col0\" as \"new_col\" FROM inputDocument
TRANSFORM_PARTITION_KEY=id
# Do you want to replicate rows staring with a specific timestamp
ENABLE_REPLICATION_POINT=false
STARTING_REPLICATION_TIMESTAMP=1659453890062043
# Replicate deletes eventually as a single-threaded process
REPLICATE_DELETES=true
# Configuration of local storage for primary and partition keys
LOCAL_STORAGE_PATH=/opt
LOCAL_STORAGE_KEYS_PER_PAGE=5000
LOCAL_STORAGE_MAX_OPEN_FILES=10000
LOCAL_STORAGE_CACHE_SIZE=104857600
LOCAL_STORAGE_BLOCK_SIZE=4096
LOCAL_STORAGE_WRITE_BUFFER_SIZE=4194304
LOCAL_STORAGE_BLOCK_RESTART_INTERVAL=16
LOCAL_STORAGE_PARTITIONS_PER_PAGE=5000
# CloudWatch support
ENABLE_CLOUD_WATCH=true
CLOUD_WATCH_REGION=us-east-1
# Preflight check
PRE_FLIGHT_CHECK=true
# Offload columns over 1MB to S3, set S3_OFFLOAD_COLUMNS to columns to offload to S3 e.g. col1
# Set S3_OFFLOAD_COLUMNS to "NONE" to disable the feature
# S3 payload stores in JSON format, e.g {"col8":"0x300bdc55eb","col1":"large payload"}
S3_OFFLOAD_COLUMNS=NONE
S3_BUCKET_NAME=my-application-bucket-2
S3_REGION=us-east-1
# Persist data on S3 with a specific prefix
# PRIMARY_KEY - s3://S3_BUCKET_NAME/KS/TBL/xxhash64(PARTITION_KEY)/xxhash64(CLUSTERING_KEY)/payload.json
# NONE - s3://S3_BUCKET_NAME/KS/TBL/xxhash64(PRIMARY_KEY|CLUSTERING_KEY)/payload.json
# Use '|' separator between partition and clustering values
S3_SHARDING_BY=PRIMARY_KEY
# NONE, or a column name
S3_USE_PRE_SIGNED_URL=col1
# text or binary
S3_PRE_SIGNED_URL_TYPE=text